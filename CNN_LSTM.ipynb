{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def fbeta(y_true, y_pred, threshold_shift=0, beta=1):\n",
    "\n",
    "   # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "   # shifting the prediction threshold from .5 if needed\n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    "\n",
    "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# Embedding\n",
    "max_features = 11000\n",
    "maxlen = 65\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('winemag_data_first150k.csv')\n",
    "\n",
    "clean_wine_df = wine_df.drop_duplicates(subset='description', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97821,), (97821, 632))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = clean_wine_df.description\n",
    "y = pd.get_dummies(clean_wine_df.variety)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [descript for descript in X]  # list of text samples\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (97821, 65)\n",
      "Shape of label tensor: (97821, 632)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "labels = np.asarray(y)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 65, 128)           1408000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 61, 64)            41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 11, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 70)                37800     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 632)               44872     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 632)               0         \n",
      "=================================================================\n",
      "Total params: 1,552,240.0\n",
      "Trainable params: 1,552,240.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(labels.shape[1]))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy', fbeta, precision, recall])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://media.giphy.com/media/l41YgdgP5Ia4niY4U/giphy.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://media.giphy.com/media/l41YgdgP5Ia4niY4U/giphy.gif')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tboard = TensorBoard(log_dir='./cnn_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70430 samples, validate on 17608 samples\n",
      "Epoch 1/20\n",
      "70430/70430 [==============================] - 14s - loss: 3.6740 - acc: 0.1628 - fbeta: 1.5647e-09 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.3294 - val_acc: 0.2276 - val_fbeta: 1.5675e-09 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "70430/70430 [==============================] - 13s - loss: 3.2415 - acc: 0.2403 - fbeta: 0.0052 - precision: 0.1178 - recall: 0.0027 - val_loss: 3.2690 - val_acc: 0.2281 - val_fbeta: 0.0097 - val_precision: 0.3992 - val_recall: 0.0049\n",
      "Epoch 3/20\n",
      "70430/70430 [==============================] - 13s - loss: 3.0398 - acc: 0.2753 - fbeta: 0.1056 - precision: 0.7096 - recall: 0.0583 - val_loss: 2.9639 - val_acc: 0.2861 - val_fbeta: 0.1521 - val_precision: 0.8000 - val_recall: 0.0845\n",
      "Epoch 4/20\n",
      "70430/70430 [==============================] - 13s - loss: 2.7477 - acc: 0.3374 - fbeta: 0.2273 - precision: 0.8136 - recall: 0.1335 - val_loss: 2.7526 - val_acc: 0.3416 - val_fbeta: 0.2341 - val_precision: 0.7807 - val_recall: 0.1383\n",
      "Epoch 5/20\n",
      "70430/70430 [==============================] - 13s - loss: 2.4797 - acc: 0.4096 - fbeta: 0.3325 - precision: 0.8162 - recall: 0.2104 - val_loss: 2.4419 - val_acc: 0.4225 - val_fbeta: 0.3679 - val_precision: 0.8190 - val_recall: 0.2382\n",
      "Epoch 6/20\n",
      "70430/70430 [==============================] - 13s - loss: 2.2876 - acc: 0.4574 - fbeta: 0.4085 - precision: 0.8194 - recall: 0.2733 - val_loss: 2.3586 - val_acc: 0.4416 - val_fbeta: 0.4098 - val_precision: 0.7835 - val_recall: 0.2785\n",
      "Epoch 7/20\n",
      "70430/70430 [==============================] - 13s - loss: 2.1378 - acc: 0.4950 - fbeta: 0.4541 - precision: 0.8266 - recall: 0.3143 - val_loss: 2.2451 - val_acc: 0.4765 - val_fbeta: 0.4448 - val_precision: 0.8125 - val_recall: 0.3072\n",
      "Epoch 8/20\n",
      "70430/70430 [==============================] - 13s - loss: 2.0065 - acc: 0.5268 - fbeta: 0.4991 - precision: 0.8353 - recall: 0.3572 - val_loss: 2.1304 - val_acc: 0.5030 - val_fbeta: 0.4767 - val_precision: 0.7997 - val_recall: 0.3407\n",
      "Epoch 9/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.8990 - acc: 0.5537 - fbeta: 0.5351 - precision: 0.8457 - recall: 0.3926 - val_loss: 2.1055 - val_acc: 0.5112 - val_fbeta: 0.4880 - val_precision: 0.8040 - val_recall: 0.3515\n",
      "Epoch 10/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.8059 - acc: 0.5766 - fbeta: 0.5644 - precision: 0.8476 - recall: 0.4243 - val_loss: 2.0677 - val_acc: 0.5164 - val_fbeta: 0.5146 - val_precision: 0.7935 - val_recall: 0.3819\n",
      "Epoch 11/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.7241 - acc: 0.5955 - fbeta: 0.5890 - precision: 0.8556 - recall: 0.4504 - val_loss: 2.0369 - val_acc: 0.5261 - val_fbeta: 0.5306 - val_precision: 0.8072 - val_recall: 0.3964\n",
      "Epoch 12/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.6516 - acc: 0.6118 - fbeta: 0.6083 - precision: 0.8563 - recall: 0.4729 - val_loss: 2.0097 - val_acc: 0.5360 - val_fbeta: 0.5410 - val_precision: 0.7965 - val_recall: 0.4107\n",
      "Epoch 13/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.5869 - acc: 0.6275 - fbeta: 0.6281 - precision: 0.8634 - recall: 0.4948 - val_loss: 2.0137 - val_acc: 0.5353 - val_fbeta: 0.5492 - val_precision: 0.7878 - val_recall: 0.4225\n",
      "Epoch 14/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.5264 - acc: 0.6418 - fbeta: 0.6432 - precision: 0.8650 - recall: 0.5130 - val_loss: 2.0396 - val_acc: 0.5416 - val_fbeta: 0.5567 - val_precision: 0.7474 - val_recall: 0.4446\n",
      "Epoch 15/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.4793 - acc: 0.6535 - fbeta: 0.6560 - precision: 0.8667 - recall: 0.5288 - val_loss: 2.0528 - val_acc: 0.5378 - val_fbeta: 0.5558 - val_precision: 0.7611 - val_recall: 0.4387\n",
      "Epoch 16/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.4335 - acc: 0.6658 - fbeta: 0.6724 - precision: 0.8712 - recall: 0.5486 - val_loss: 2.1629 - val_acc: 0.5269 - val_fbeta: 0.5456 - val_precision: 0.7110 - val_recall: 0.4435\n",
      "Epoch 17/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.3922 - acc: 0.6765 - fbeta: 0.6834 - precision: 0.8733 - recall: 0.5624 - val_loss: 2.0732 - val_acc: 0.5360 - val_fbeta: 0.5566 - val_precision: 0.7626 - val_recall: 0.4391\n",
      "Epoch 18/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.3545 - acc: 0.6865 - fbeta: 0.6942 - precision: 0.8769 - recall: 0.5755 - val_loss: 2.0816 - val_acc: 0.5441 - val_fbeta: 0.5675 - val_precision: 0.7382 - val_recall: 0.4619\n",
      "Epoch 19/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.3221 - acc: 0.6969 - fbeta: 0.7043 - precision: 0.8795 - recall: 0.5884 - val_loss: 2.1462 - val_acc: 0.5356 - val_fbeta: 0.5591 - val_precision: 0.7194 - val_recall: 0.4581\n",
      "Epoch 20/20\n",
      "70430/70430 [==============================] - 13s - loss: 1.2882 - acc: 0.7037 - fbeta: 0.7134 - precision: 0.8810 - recall: 0.6004 - val_loss: 2.1678 - val_acc: 0.5336 - val_fbeta: 0.5533 - val_precision: 0.7229 - val_recall: 0.4490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd61c572d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[tboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9344/9783 [===========================>..] - ETA: 0sTest loss: 2.19062770967\n",
      "Test accuracy: 0.534396401916\n",
      "Test fbeta: 0.555897237199\n",
      "Test Precision: 0.718452677074\n",
      "Test Recall: 0.454052948972\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=128)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test fbeta:', score[2])\n",
    "print('Test Precision:', score[3])\n",
    "print('Test Recall:', score[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I don't know how, but yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70430 samples, validate on 17608 samples\n",
      "Epoch 1/10\n",
      "70430/70430 [==============================] - 13s - loss: 1.2634 - acc: 0.7118 - fbeta: 0.7233 - precision: 0.8848 - recall: 0.6126 - val_loss: 2.1963 - val_acc: 0.5349 - val_fbeta: 0.5559 - val_precision: 0.7070 - val_recall: 0.4587\n",
      "Epoch 2/10\n",
      "70430/70430 [==============================] - 13s - loss: 1.2335 - acc: 0.7193 - fbeta: 0.7307 - precision: 0.8854 - recall: 0.6230 - val_loss: 2.1528 - val_acc: 0.5325 - val_fbeta: 0.5583 - val_precision: 0.7426 - val_recall: 0.4483\n",
      "Epoch 3/10\n",
      "70430/70430 [==============================] - 13s - loss: 1.2088 - acc: 0.7274 - fbeta: 0.7379 - precision: 0.8880 - recall: 0.6322 - val_loss: 2.1888 - val_acc: 0.5351 - val_fbeta: 0.5600 - val_precision: 0.7155 - val_recall: 0.4609\n",
      "Epoch 4/10\n",
      "70430/70430 [==============================] - 13s - loss: 1.1791 - acc: 0.7335 - fbeta: 0.7438 - precision: 0.8884 - recall: 0.6406 - val_loss: 2.2479 - val_acc: 0.5384 - val_fbeta: 0.5620 - val_precision: 0.6952 - val_recall: 0.4724\n",
      "Epoch 5/10\n",
      "70430/70430 [==============================] - 13s - loss: 1.1566 - acc: 0.7384 - fbeta: 0.7515 - precision: 0.8912 - recall: 0.6505 - val_loss: 2.2620 - val_acc: 0.5292 - val_fbeta: 0.5563 - val_precision: 0.6977 - val_recall: 0.4633\n",
      "Epoch 6/10\n",
      "70430/70430 [==============================] - 13s - loss: 1.1350 - acc: 0.7467 - fbeta: 0.7588 - precision: 0.8941 - recall: 0.6599 - val_loss: 2.2819 - val_acc: 0.5307 - val_fbeta: 0.5534 - val_precision: 0.6861 - val_recall: 0.4644\n",
      "Epoch 7/10\n",
      "70430/70430 [==============================] - 13s - loss: 1.1173 - acc: 0.7505 - fbeta: 0.7618 - precision: 0.8930 - recall: 0.6650 - val_loss: 2.3499 - val_acc: 0.5305 - val_fbeta: 0.5556 - val_precision: 0.6797 - val_recall: 0.4707\n",
      "Epoch 8/10\n",
      "70430/70430 [==============================] - 13s - loss: 1.0931 - acc: 0.7575 - fbeta: 0.7696 - precision: 0.8993 - recall: 0.6734 - val_loss: 2.3285 - val_acc: 0.5365 - val_fbeta: 0.5612 - val_precision: 0.6718 - val_recall: 0.4826\n",
      "Epoch 9/10\n",
      "70430/70430 [==============================] - 13s - loss: 1.0766 - acc: 0.7626 - fbeta: 0.7752 - precision: 0.8997 - recall: 0.6818 - val_loss: 2.3095 - val_acc: 0.5352 - val_fbeta: 0.5606 - val_precision: 0.6817 - val_recall: 0.4768\n",
      "Epoch 10/10\n",
      "70430/70430 [==============================] - 13s - loss: 1.0467 - acc: 0.7672 - fbeta: 0.7794 - precision: 0.8999 - recall: 0.6880 - val_loss: 2.3006 - val_acc: 0.5257 - val_fbeta: 0.5537 - val_precision: 0.7013 - val_recall: 0.4583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd6243910b8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9344/9783 [===========================>..] - ETA: 0sTest loss: 2.30032086945\n",
      "Test accuracy: 0.529898803999\n",
      "Test fbeta: 0.55713296237\n",
      "Test Precision: 0.697359951169\n",
      "Test Recall: 0.464274762279\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=128)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test fbeta:', score[2])\n",
    "print('Test Precision:', score[3])\n",
    "print('Test Recall:', score[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Rabbit Holes and Recommendations\n",
    "\n",
    "- Balancing Class weights in Keras.\n",
    "- Visualizing Embeddings in Tensorflow.\n",
    "- Wondering why I chose a dataset with 632 classes.\n",
    "- Loving that I chose a dataset with 632 classes.\n",
    "- Fine Tune CNN/LSTM model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
