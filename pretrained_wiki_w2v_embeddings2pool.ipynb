{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 words excluded from [1000x300] embedding matrix:7.20%\n",
      "\n",
      "182 words excluded from [2000x300] embedding matrix:9.10%\n",
      "\n",
      "311 words excluded from [3000x300] embedding matrix:10.37%\n",
      "\n",
      "469 words excluded from [4000x300] embedding matrix:11.72%\n",
      "\n",
      "645 words excluded from [5000x300] embedding matrix:12.90%\n",
      "\n",
      "830 words excluded from [6000x300] embedding matrix:13.83%\n",
      "\n",
      "1049 words excluded from [7000x300] embedding matrix:14.99%\n",
      "\n",
      "1287 words excluded from [8000x300] embedding matrix:16.09%\n",
      "\n",
      "1548 words excluded from [9000x300] embedding matrix:17.20%\n",
      "\n",
      "1805 words excluded from [10000x300] embedding matrix:18.05%\n",
      "\n",
      "2111 words excluded from [11000x300] embedding matrix:19.19%\n",
      "\n",
      "2457 words excluded from [12000x300] embedding matrix:20.47%\n",
      "\n",
      "2769 words excluded from [13000x300] embedding matrix:21.30%\n",
      "\n",
      "3155 words excluded from [14000x300] embedding matrix:22.54%\n",
      "\n",
      "3534 words excluded from [15000x300] embedding matrix:23.56%\n",
      "\n",
      "3922 words excluded from [16000x300] embedding matrix:24.51%\n",
      "\n",
      "4353 words excluded from [17000x300] embedding matrix:25.61%\n",
      "\n",
      "4826 words excluded from [18000x300] embedding matrix:26.81%\n",
      "\n",
      "5268 words excluded from [19000x300] embedding matrix:27.73%\n",
      "\n",
      "5747 words excluded from [20000x300] embedding matrix:28.73%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for r in np.arange(1000,21000,1000):\n",
    "#     MAX_NB_WORDS = r\n",
    "#     # prepare embedding matrix\n",
    "#     num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "#     embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "#     for word, i in word_index.items():\n",
    "#         if i >= MAX_NB_WORDS:\n",
    "#             continue\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             # words not found in embedding index will be all-zeros.\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#     # load pre-trained word embeddings into an Embedding layer\n",
    "#     # note that we set trainable = False so as to keep the embeddings fixed\n",
    "#     embedding_layer = Embedding(num_words,\n",
    "#                                 EMBEDDING_DIM,\n",
    "#                                 weights=[embedding_matrix],\n",
    "#                                 input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                                 trainable=False)\n",
    "#     b = np.sum(np.argmax(embedding_matrix, axis=1) == 0)\n",
    "\n",
    "#     print(\"{} words excluded from [{}x300] embedding matrix:{:.2f}%\".format(b, r, (b/r)*100))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def fbeta(y_true, y_pred, threshold_shift=0, beta=1):\n",
    "\n",
    "   # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "   # shifting the prediction threshold from .5 if needed\n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    "\n",
    "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 174015 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "BASE_DIR = '/home/adamszabunio'\n",
    "MAX_SEQUENCE_LENGTH = 80\n",
    "MAX_NB_WORDS = 11000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(BASE_DIR, 'deps.words'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('winemag_data_first150k.csv')\n",
    "\n",
    "clean_wine_df = wine_df.drop_duplicates(subset='description', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97821,), (97821, 632))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = clean_wine_df.description\n",
    "y = pd.get_dummies(clean_wine_df.variety)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [descript for descript in X]\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (97821, 80)\n",
      "Shape of label tensor: (97821, 632)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "labels = np.asarray(y)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   6, 1693,  400,  415,    8, 3007,   24, 2603,    1,  331,  306,\n",
       "        119,  596,   64,   10,   40,  105,   38,   22,   13,    1,    3,\n",
       "       1153,  172,    4,  294, 3179,    2,   23,  809,   30,  139,  102,\n",
       "         21,    1,    3,  367,  604, 1287,   10,    2,  697,   86,    1,\n",
       "       2255,   24,  672,   12,   18,   11,   26,   64, 1804,    4,   11,\n",
       "         12,  384,  960, 1298,  538,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Excluding 26143 embeddings. These embeddings will be all zeros\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "count = 0\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        count += 1\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print(\"Excluding {} embeddings. These embeddings will be all zeros\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 80, 300)           3300000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 76, 128)           192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 11, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 632)               81528     \n",
      "=================================================================\n",
      "Total params: 3,688,600.0\n",
      "Trainable params: 388,600.0\n",
      "Non-trainable params: 3,300,000.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(y.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc', fbeta, precision, recall])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_callback = TensorBoard(log_dir='./w2vec2pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78256 samples, validate on 19565 samples\n",
      "Epoch 1/20\n",
      "78256/78256 [==============================] - 15s - loss: 3.8500 - acc: 0.1176 - fbeta: 1.0144e-04 - precision: 0.0065 - recall: 5.1114e-05 - val_loss: 3.6043 - val_acc: 0.1815 - val_fbeta: 0.0112 - val_precision: 0.5213 - val_recall: 0.0057\n",
      "Epoch 2/20\n",
      "78256/78256 [==============================] - 14s - loss: 3.1982 - acc: 0.2540 - fbeta: 0.1277 - precision: 0.8018 - recall: 0.0714 - val_loss: 2.9661 - val_acc: 0.3117 - val_fbeta: 0.2441 - val_precision: 0.7825 - val_recall: 0.1454\n",
      "Epoch 3/20\n",
      "78256/78256 [==============================] - 14s - loss: 2.7154 - acc: 0.3580 - fbeta: 0.2963 - precision: 0.8331 - recall: 0.1816 - val_loss: 2.6914 - val_acc: 0.3616 - val_fbeta: 0.3169 - val_precision: 0.7859 - val_recall: 0.1993\n",
      "Epoch 4/20\n",
      "78256/78256 [==============================] - 14s - loss: 2.4767 - acc: 0.4061 - fbeta: 0.3568 - precision: 0.8355 - recall: 0.2285 - val_loss: 2.5563 - val_acc: 0.4015 - val_fbeta: 0.3305 - val_precision: 0.8574 - val_recall: 0.2056\n",
      "Epoch 5/20\n",
      "78256/78256 [==============================] - 14s - loss: 2.3069 - acc: 0.4426 - fbeta: 0.3988 - precision: 0.8468 - recall: 0.2623 - val_loss: 2.3816 - val_acc: 0.4405 - val_fbeta: 0.3879 - val_precision: 0.8495 - val_recall: 0.2524\n",
      "Epoch 6/20\n",
      "78256/78256 [==============================] - 14s - loss: 2.1767 - acc: 0.4708 - fbeta: 0.4375 - precision: 0.8511 - recall: 0.2958 - val_loss: 2.3571 - val_acc: 0.4470 - val_fbeta: 0.4118 - val_precision: 0.8200 - val_recall: 0.2762\n",
      "Epoch 7/20\n",
      "78256/78256 [==============================] - 14s - loss: 2.0759 - acc: 0.4926 - fbeta: 0.4667 - precision: 0.8570 - recall: 0.3223 - val_loss: 2.3066 - val_acc: 0.4687 - val_fbeta: 0.4539 - val_precision: 0.7835 - val_recall: 0.3204\n",
      "Epoch 8/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.9925 - acc: 0.5128 - fbeta: 0.4930 - precision: 0.8622 - recall: 0.3470 - val_loss: 2.2557 - val_acc: 0.4753 - val_fbeta: 0.4592 - val_precision: 0.8149 - val_recall: 0.3206\n",
      "Epoch 9/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.9208 - acc: 0.5263 - fbeta: 0.5133 - precision: 0.8605 - recall: 0.3675 - val_loss: 2.2557 - val_acc: 0.4770 - val_fbeta: 0.4526 - val_precision: 0.8241 - val_recall: 0.3131\n",
      "Epoch 10/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.8624 - acc: 0.5383 - fbeta: 0.5318 - precision: 0.8653 - recall: 0.3855 - val_loss: 2.3149 - val_acc: 0.4782 - val_fbeta: 0.4793 - val_precision: 0.7670 - val_recall: 0.3495\n",
      "Epoch 11/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.8083 - acc: 0.5498 - fbeta: 0.5462 - precision: 0.8669 - recall: 0.4006 - val_loss: 2.2350 - val_acc: 0.4921 - val_fbeta: 0.4659 - val_precision: 0.8233 - val_recall: 0.3259\n",
      "Epoch 12/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.7539 - acc: 0.5595 - fbeta: 0.5615 - precision: 0.8669 - recall: 0.4170 - val_loss: 2.3214 - val_acc: 0.4915 - val_fbeta: 0.4841 - val_precision: 0.7747 - val_recall: 0.3531\n",
      "Epoch 13/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.7074 - acc: 0.5696 - fbeta: 0.5756 - precision: 0.8712 - recall: 0.4316 - val_loss: 2.2774 - val_acc: 0.4946 - val_fbeta: 0.4801 - val_precision: 0.7869 - val_recall: 0.3464\n",
      "Epoch 14/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.6686 - acc: 0.5767 - fbeta: 0.5869 - precision: 0.8725 - recall: 0.4443 - val_loss: 2.6255 - val_acc: 0.4874 - val_fbeta: 0.5003 - val_precision: 0.6761 - val_recall: 0.3978\n",
      "Epoch 15/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.6234 - acc: 0.5864 - fbeta: 0.5985 - precision: 0.8764 - recall: 0.4563 - val_loss: 2.4388 - val_acc: 0.4563 - val_fbeta: 0.4536 - val_precision: 0.7738 - val_recall: 0.3219\n",
      "Epoch 16/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.5881 - acc: 0.5950 - fbeta: 0.6099 - precision: 0.8760 - recall: 0.4696 - val_loss: 2.4773 - val_acc: 0.4942 - val_fbeta: 0.5023 - val_precision: 0.7302 - val_recall: 0.3837\n",
      "Epoch 17/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.5513 - acc: 0.6027 - fbeta: 0.6201 - precision: 0.8801 - recall: 0.4806 - val_loss: 2.6830 - val_acc: 0.4873 - val_fbeta: 0.4940 - val_precision: 0.6850 - val_recall: 0.3871\n",
      "Epoch 18/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.5176 - acc: 0.6103 - fbeta: 0.6289 - precision: 0.8807 - recall: 0.4909 - val_loss: 2.5714 - val_acc: 0.4848 - val_fbeta: 0.5013 - val_precision: 0.7068 - val_recall: 0.3892\n",
      "Epoch 19/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.4828 - acc: 0.6200 - fbeta: 0.6399 - precision: 0.8828 - recall: 0.5037 - val_loss: 2.6008 - val_acc: 0.4870 - val_fbeta: 0.5006 - val_precision: 0.7099 - val_recall: 0.3876\n",
      "Epoch 20/20\n",
      "78256/78256 [==============================] - 14s - loss: 1.4518 - acc: 0.6270 - fbeta: 0.6492 - precision: 0.8853 - recall: 0.5143 - val_loss: 2.5849 - val_acc: 0.4693 - val_fbeta: 0.4820 - val_precision: 0.7152 - val_recall: 0.3643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2e6269470>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19200/19565 [============================>.] - ETA: 0sTest loss: 2.58494032372\n",
      "Test accuracy: 0.469307436752\n",
      "Test fbeta: 0.48195039188\n",
      "Test Precision: 0.715242565227\n",
      "Test Recall: 0.364272936325\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=128)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test fbeta:', score[2])\n",
    "print('Test Precision:', score[3])\n",
    "print('Test Recall:', score[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
